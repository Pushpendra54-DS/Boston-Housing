# Boston Housing â€” Regression Models
# Boston Housing â€” Regression Models (Linear, Random Forest, XGBoost)

This project predicts Boston house prices using three machine learning models:  
**Linear Regression, Random Forest, and XGBoost**.
---

## Motivation

I wanted to compare how three different regression modelsâ€”Linear Regression, Random Forest, and XGBoostâ€”perform in predicting Boston housing prices, while also learning how to structure and document a data science project clearly.

---

## ðŸ“‚ Project Structure
â”œâ”€â”€ data/ # dataset(s) or link
â”œâ”€â”€ notebooks/ # Jupyter notebooks (EDA & models)
â”œâ”€â”€ scripts/ # Python scripts (training, prediction)
â”œâ”€â”€ results/ # Plots & evaluation metrics
â”œâ”€â”€ requirements.txt # Project dependencies
â”œâ”€â”€ .gitignore # Ignore unnecessary files
â””â”€â”€ README.md

## Data Used

- Dataset: [Dataset: boston.csv](./boston.csv)


---

## ðŸ§° Tech Stack
- Python  
- pandas, numpy  
- scikit-learn (Linear Regression, Random Forest)  
- xgboost  
- matplotlib, seaborn (visualization)  
- joblib (model saving)

---

## ðŸ“Š Models and Metrics
We trained and evaluated three models:

| Model           | RÂ² (test) | MAE  |  MSE |
|-----------------|-----------|------|------|
| Linear          | 0.67      | 3.20 | 24.49|
| Random Forest   | 0.92      | 1.89 | 6.20 |
| XGBoost         | 0.92      | 1.89 | 6.20 |




---

## ðŸ–¼ Visuals
Plots will be stored in the `results/` folder.  
Examples:  
- Actual vs Predicted  
- Feature Importance  

---

## How to Run or View

###  Option 1: View on GitHub
Click any notebook in the `notebooks/` folder to open it directly in your browserâ€”no setup needed.

###  Option 2: Run Locally
1. Clone this repository:
   ```bash
   git clone https://github.com/Pushpendra54-DS/Bonston-Housing.git

- Navigate into it:cd Bonston-Housing
- Install required packages:pip install -r requirements.txt
- Launch Jupyter Notebook:jupyter notebook
- Browse to the notebooks/ folder and click to run any notebook.

---
 
## Future Work

- Add cross-validation and hyperparameter tuning to improve model generalization.
- Visualize feature importance and residuals for better model interpretation.
- Build a simple web interface (e.g., with Streamlit or Flask) to make predictions interactively.
- Experiment with neural network models or ensemble stacking for performance comparison.






